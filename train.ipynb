{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dataset.dataset import NoCDataset\n",
    "from model.vanilla import VanillaModel\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Samples = 980\n",
      "Graph(num_nodes=31, num_edges=40,\n",
      "      ndata_schemes={'delay': Scheme(shape=(), dtype=torch.float64), 'in_latency': Scheme(shape=(), dtype=torch.int64), 'out_latency': Scheme(shape=(), dtype=torch.int64), 'op_type': Scheme(shape=(4,), dtype=torch.int64)}\n",
      "      edata_schemes={'size': Scheme(shape=(), dtype=torch.float64), 'cnt': Scheme(shape=(), dtype=torch.float64), 'route': Scheme(shape=(), dtype=torch.int64)})\n"
     ]
    }
   ],
   "source": [
    "dataset = NoCDataset()\n",
    "print(f\"#Samples = {len(dataset)}\")\n",
    "print(dataset[0])\n",
    "\n",
    "# TODO: use advanced dgl.dataloader instead of manually dividing dataset\n",
    "num_training = 0.9 * len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0; loss = 37109733785600.0\n",
      "iteration: 50; loss = 6.909738231844045e+16\n",
      "iteration: 100; loss = 4531420814901248.0\n",
      "iteration: 150; loss = 6.514671815514653e+18\n",
      "iteration: 200; loss = 3222119093633024.0\n",
      "iteration: 250; loss = 2.1475583708102237e+22\n",
      "iteration: 300; loss = 3.5374218164530315e+19\n",
      "iteration: 350; loss = 19088753557504.0\n",
      "iteration: 400; loss = 3.424808076317609e+19\n",
      "iteration: 450; loss = 5.1390836756814234e+17\n",
      "iteration: 500; loss = 4.4445379858949734e+17\n",
      "iteration: 550; loss = 4.825674070936781e+17\n",
      "iteration: 600; loss = 36623093760.0\n",
      "iteration: 650; loss = 6.247359098325565e+17\n",
      "iteration: 700; loss = 16848318889984.0\n",
      "iteration: 750; loss = 2116576346112.0\n",
      "iteration: 800; loss = 5.458869502974362e+16\n",
      "iteration: 850; loss = 7835879473152.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:12<03:59, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0; loss = 95993668304896.0\n",
      "iteration: 50; loss = 3.465037665743667e+16\n",
      "iteration: 100; loss = 1778385298653184.0\n",
      "iteration: 150; loss = 6.366141538497462e+18\n",
      "iteration: 200; loss = 2308832952844288.0\n",
      "iteration: 250; loss = 2.389983085511913e+22\n",
      "iteration: 300; loss = 3.4838155668454638e+19\n",
      "iteration: 350; loss = 20359149518848.0\n",
      "iteration: 400; loss = 3.073441543475744e+19\n",
      "iteration: 450; loss = 5.085232344932024e+17\n",
      "iteration: 500; loss = 4.415020221857792e+17\n",
      "iteration: 550; loss = 4.791061444894392e+17\n",
      "iteration: 600; loss = 35908726784.0\n",
      "iteration: 650; loss = 4.680845368533975e+17\n",
      "iteration: 700; loss = 6603489148928.0\n",
      "iteration: 750; loss = 1025789263872.0\n",
      "iteration: 800; loss = 2.6814580340555776e+16\n",
      "iteration: 850; loss = 3413015789568.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:24<03:36, 12.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0; loss = 45022682546176.0\n",
      "iteration: 50; loss = 1.876416450789376e+16\n",
      "iteration: 100; loss = 603643023720448.0\n",
      "iteration: 150; loss = 3.552492382530306e+18\n",
      "iteration: 200; loss = 590801675485184.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:27<04:05, 13.62s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xuechenhao/gnn4noc/train.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2256313030227d/home/xuechenhao/gnn4noc/train.ipynb#ch0000002vscode-remote?line=26'>27</a>\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(pred, in_latency)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2256313030227d/home/xuechenhao/gnn4noc/train.ipynb#ch0000002vscode-remote?line=27'>28</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2256313030227d/home/xuechenhao/gnn4noc/train.ipynb#ch0000002vscode-remote?line=28'>29</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2256313030227d/home/xuechenhao/gnn4noc/train.ipynb#ch0000002vscode-remote?line=29'>30</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2256313030227d/home/xuechenhao/gnn4noc/train.ipynb#ch0000002vscode-remote?line=31'>32</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/_tensor.py?line=386'>387</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/_tensor.py?line=387'>388</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/_tensor.py?line=388'>389</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/_tensor.py?line=389'>390</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/_tensor.py?line=393'>394</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/_tensor.py?line=394'>395</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/_tensor.py?line=395'>396</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///~/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "\n",
    "model = VanillaModel(n_feats=5, e_feats=3, h_feats=100).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "\n",
    "epoches = 20\n",
    "for e in tqdm(range(epoches)):\n",
    "    for i, g in enumerate(dataset):\n",
    "        if i > num_training:\n",
    "            break\n",
    "\n",
    "        #TODO: merge feature extraction to NoCDataset\n",
    "        g = g.to(device)\n",
    "        nfeat = torch.concat([\n",
    "            g.ndata[\"delay\"].reshape(-1, 1).float(), \n",
    "            g.ndata[\"op_type\"].float()\n",
    "            ], dim=1)\n",
    "        efeat = torch.concat([\n",
    "            g.edata[\"size\"].reshape(-1, 1).float(),\n",
    "            g.edata[\"cnt\"].reshape(-1, 1).float(),\n",
    "            g.edata[\"route\"].reshape(-1, 1).float(),\n",
    "            ], dim=1)\n",
    "        in_latency = g.ndata[\"in_latency\"].float()\n",
    "\n",
    "        pred = model(g, nfeat, efeat).squeeze()\n",
    "        pred = pred * g.ndata[\"op_type\"][:, 2]  # mask non-worker\n",
    "        loss = F.mse_loss(pred, in_latency)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"iteration: {i}; loss = {loss}\")\n",
    "            # print(f\"in_latency = {in_latency}\")\n",
    "            # print(f\"pred = {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1151, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(36.4881, device='cuda:0')\n",
      "tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(56.5383, device='cuda:0')\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(52.1333, device='cuda:0')\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(24.1608, device='cuda:0')\n",
      "tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(37.5766, device='cuda:0')\n",
      "tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(41.8914, device='cuda:0')\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(55.6862, device='cuda:0')\n",
      "tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(88.2572, device='cuda:0')\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(37.9032, device='cuda:0')\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(43.5961, device='cuda:0')\n",
      "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(67.8489, device='cuda:0')\n",
      "tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(44.8679, device='cuda:0')\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(34.5546, device='cuda:0')\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(23.4890, device='cuda:0')\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(32.0750, device='cuda:0')\n",
      "tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(41.5460, device='cuda:0')\n",
      "tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(28.2729, device='cuda:0')\n",
      "tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(82.8131, device='cuda:0')\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(67.8201, device='cuda:0')\n",
      "tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(79.5003, device='cuda:0')\n",
      "tensor(0.1041, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(88.5985, device='cuda:0')\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(46.8251, device='cuda:0')\n",
      "tensor(332.2429, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(36.1276, device='cuda:0')\n",
      "tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(32.8663, device='cuda:0')\n",
      "tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(67.8309, device='cuda:0')\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(41.8518, device='cuda:0')\n",
      "tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(101.5815, device='cuda:0')\n",
      "tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(47.9802, device='cuda:0')\n",
      "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(23.9281, device='cuda:0')\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(46.2202, device='cuda:0')\n",
      "tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(34.7597, device='cuda:0')\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(54.6185, device='cuda:0')\n",
      "tensor(1.7524, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(20.9969, device='cuda:0')\n",
      "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(34.1834, device='cuda:0')\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(96.8056, device='cuda:0')\n",
      "tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(45.1152, device='cuda:0')\n",
      "tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(48.3042, device='cuda:0')\n",
      "tensor(0.0274, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(39.1855, device='cuda:0')\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(49.6217, device='cuda:0')\n",
      "tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(47.7639, device='cuda:0')\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(49.4002, device='cuda:0')\n",
      "tensor(0.0877, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(83.1192, device='cuda:0')\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(31.1346, device='cuda:0')\n",
      "tensor(0.1699, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(59.5327, device='cuda:0')\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(46.4144, device='cuda:0')\n",
      "tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(32.6568, device='cuda:0')\n",
      "tensor(0.0493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(63.2650, device='cuda:0')\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(20.7659, device='cuda:0')\n",
      "tensor(43.6199, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(24.1987, device='cuda:0')\n",
      "tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(39.1178, device='cuda:0')\n",
      "tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(49.3844, device='cuda:0')\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(63.0943, device='cuda:0')\n",
      "tensor(0.0603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(36.9618, device='cuda:0')\n",
      "tensor(18526.4492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(36.1100, device='cuda:0')\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(39.1938, device='cuda:0')\n",
      "tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(45.6631, device='cuda:0')\n",
      "tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(38.7074, device='cuda:0')\n",
      "tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(20.8985, device='cuda:0')\n",
      "tensor(0.0986, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(46.4749, device='cuda:0')\n",
      "tensor(0.1096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(56.8210, device='cuda:0')\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(45.9441, device='cuda:0')\n",
      "tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(63.0553, device='cuda:0')\n",
      "tensor(0.0932, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(104.6066, device='cuda:0')\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(36.1260, device='cuda:0')\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(54.1869, device='cuda:0')\n",
      "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(58.9757, device='cuda:0')\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(49.0670, device='cuda:0')\n",
      "tensor(0.1315, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(82.7165, device='cuda:0')\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(34.7533, device='cuda:0')\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(32.5394, device='cuda:0')\n",
      "tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(65.3040, device='cuda:0')\n",
      "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(35.8187, device='cuda:0')\n",
      "tensor(0.1973, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(56.9942, device='cuda:0')\n",
      "tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(61.2492, device='cuda:0')\n",
      "tensor(0.0110, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(92.6090, device='cuda:0')\n",
      "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(88.7279, device='cuda:0')\n",
      "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(49.6622, device='cuda:0')\n",
      "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(24.5626, device='cuda:0')\n",
      "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(83.0335, device='cuda:0')\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(38.7397, device='cuda:0')\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(25.9987, device='cuda:0')\n",
      "tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(34.9905, device='cuda:0')\n",
      "tensor(0.0712, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(42.3844, device='cuda:0')\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(28.4854, device='cuda:0')\n",
      "tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(59.5206, device='cuda:0')\n",
      "tensor(0.1260, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(54.1848, device='cuda:0')\n",
      "tensor(36.8874, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(44.2112, device='cuda:0')\n",
      "tensor(0.0219, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(31.6646, device='cuda:0')\n",
      "tensor(0.0164, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(20.7782, device='cuda:0')\n",
      "tensor(0.0658, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(36.3144, device='cuda:0')\n",
      "tensor(0.0438, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(31.1271, device='cuda:0')\n",
      "tensor(0.0548, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(70.5419, device='cuda:0')\n",
      "tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(82.3368, device='cuda:0')\n",
      "tensor(0.0329, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(66.5278, device='cuda:0')\n",
      "tensor(0.0767, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(56.6406, device='cuda:0')\n",
      "tensor(1.1101, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(18.0827, device='cuda:0')\n",
      "tensor(0.0822, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(82.5293, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "# we use relative error to measure\n",
    "\n",
    "for i, g in enumerate(dataset):\n",
    "    if i <= num_training:\n",
    "        continue\n",
    "\n",
    "\n",
    "    g = g.to(device)\n",
    "    nfeat = torch.concat([\n",
    "        g.ndata[\"delay\"].reshape(-1, 1).float(), \n",
    "        g.ndata[\"op_type\"].float()\n",
    "        ], dim=1).to(device)\n",
    "    efeat = torch.concat([\n",
    "        g.edata[\"size\"].reshape(-1, 1).float(),\n",
    "        g.edata[\"cnt\"].reshape(-1, 1).float(),\n",
    "        g.edata[\"route\"].reshape(-1, 1).float(),\n",
    "        ], dim=1).to(device)\n",
    "    in_latency = torch.log(g.ndata[\"in_latency\"].float() + 1).to(device)\n",
    "\n",
    "    pred = model(g, nfeat, efeat).squeeze()\n",
    "    print(sum(pred))\n",
    "    print(sum(in_latency))\n",
    "    # loss = F.mse_loss(pred, in_latency)\n",
    "    # print(loss)\n",
    "\n",
    "    pred = torch.exp(pred)\n",
    "    # print(in_latency)\n",
    "    # print(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96bcc5b7e4704e2a96f47d0e0ab54a35b61edd90a78c04f0a0f415e769481874"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('gnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
